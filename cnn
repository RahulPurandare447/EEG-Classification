import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset
import mne
from sklearn.metrics import accuracy_score

# Function to load and preprocess EEG data
def preprocess_folder(folder_path, sampling_rate=128):
    all_data_segments = []
    all_labels = []
    
    for filename in os.listdir(folder_path):
        if filename.endswith(".edf"):
            file_path = os.path.join(folder_path, filename)
            print(f"Processing {file_path}")

            # Load the raw EEG data from the EDF file
            raw = mne.io.read_raw_edf(file_path, preload=True)
            raw.pick_types(eeg=True)

            # Apply filters
            raw.filter(l_freq=1.0, h_freq=40.0)  # Bandpass filter
            raw.resample(sampling_rate)

            # Set EEG reference
            raw.set_eeg_reference('average', projection=True)  # Use average reference
            raw.apply_proj()  # Apply the projection

            # Get the data as a NumPy array
            data = raw.get_data()

            # Create segments (epochs)
            segment_samples = 5 * sampling_rate  # 5 seconds of data
            step = segment_samples  # No overlap
            segments = []

            for start in range(0, data.shape[1] - segment_samples + 1, step):
                end = start + segment_samples
                segment = data[:, start:end]
                segments.append(segment)

            # Normalize each segment
            segments = np.array(segments)
            all_data_segments.extend(segments)

            # Assign label based on filename pattern (adjust this according to your dataset)
            label = 0 if 'h' in filename.lower() else 1  # Example: healthy = 0, schizophrenia = 1
            all_labels.extend([label] * len(segments))

    # Convert to NumPy arrays
    X = np.array(all_data_segments)  # Shape: (num_epochs, channels, segment_samples)
    y = np.array(all_labels)  # Labels for each segment
    return X, y

# Specify your folder path containing the EDF files
folder_path = "C:/Users/rapur/Downloads/dataverse_files"
X, y = preprocess_folder(folder_path)

# Step 1: Reshape and split the data into training and validation sets
X = X.transpose(0, 2, 1)  # Shape: (num_epochs, segment_samples, channels)

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Convert to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)  # Add channel dimension (1 for single channel)
y_train_tensor = torch.tensor(y_train, dtype=torch.long)
X_val_tensor = torch.tensor(X_val, dtype=torch.float32).unsqueeze(1)  # Add channel dimension (1 for single channel)
y_val_tensor = torch.tensor(y_val, dtype=torch.long)

# Step 2: Create DataLoader for batching
batch_size = 16  # Adjust batch size if needed
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
val_dataset = TensorDataset(X_val_tensor, y_val_tensor)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# Step 3: Define the CNN model with Global Average Pooling (GAP)
class EEGCNNClassifier(nn.Module):
    def _init_(self, input_channels, input_size):
        super(EEGCNNClassifier, self)._init_()

        # Convolutional layers
        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=(3, 3), padding=1)  # Reduced filters
        self.conv2 = nn.Conv2d(16, 32, kernel_size=(3, 3), padding=1)  # Reduced filters
        self.pool = nn.MaxPool2d(2, 2)  # Pooling to reduce size

        # Global Average Pooling layer to reduce spatial dimensions to 1x1
        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)  # Reduces each feature map to 1x1

        # Fully connected layers
        self.fc1 = nn.Linear(32, 64)  # 32 is the number of channels after the final convolution
        self.fc2 = nn.Linear(64, 2)   # 2 classes (healthy, schizophrenia)

        self.relu = nn.ReLU()
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.pool(x)
        x = self.relu(self.conv2(x))
        x = self.pool(x)

        # Apply Global Average Pooling
        x = self.global_avg_pool(x)
        x = torch.flatten(x, 1)  # Flatten to 1D for the fully connected layer

        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        x = self.softmax(x)

        return x

# Step 4: Initialize the model, loss function, and optimizer
input_size = X_train_tensor.shape[2]  # Number of channels
model = EEGCNNClassifier(input_channels=1, input_size=input_size)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Step 5: Train the model
epochs = 50
for epoch in range(epochs):
    model.train()  # Set the model to training mode
    running_loss = 0.0
    correct_train = 0
    total_train = 0

    for inputs, labels in train_loader:
        optimizer.zero_grad()
        
        # Forward pass
        outputs = model(inputs)  # Get model predictions
        loss = criterion(outputs, labels)  # Compute loss
        loss.backward()  # Backpropagation
        optimizer.step()  # Update weights

        # Calculate training accuracy
        _, predicted = torch.max(outputs, 1)
        correct_train += (predicted == labels).sum().item()
        total_train += labels.size(0)
        running_loss += loss.item()

    # Calculate training accuracy for this epoch
    train_accuracy = correct_train / total_train

    # Validation phase
    model.eval()  # Set model to evaluation mode
    correct_val = 0
    total_val = 0
    running_val_loss = 0.0

    with torch.no_grad():
        for inputs, labels in val_loader:
            val_outputs = model(inputs)
            val_loss = criterion(val_outputs, labels)
            running_val_loss += val_loss.item()

            # Validation accuracy
            _, val_predicted = torch.max(val_outputs, 1)
            correct_val += (val_predicted == labels).sum().item()
            total_val += labels.size(0)

    # Calculate validation accuracy
    val_accuracy = correct_val / total_val

    # Print the loss and accuracy statistics every 10 epochs
    if (epoch + 1) % 10 == 0:
        print(f"Epoch [{epoch+1}/{epochs}], "
              f"Train Loss: {running_loss/len(train_loader):.4f}, Train Acc: {train_accuracy:.4f}, "
              f"Val Loss: {running_val_loss/len(val_loader):.4f}, Val Acc: {val_accuracy:.4f}")

print("Training complete.")
